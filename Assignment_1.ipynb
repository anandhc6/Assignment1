{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandhc6/Assignment1/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_iRYe-nB_As"
      },
      "outputs": [],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from keras.datasets import fashion_mnist\n",
        "#import the required libraries\n",
        "from matplotlib import pyplot\n",
        "import math as math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(X, y), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Reshaping the data \n",
        "X = X.reshape(X.shape[0], 784)\n",
        "X_test = X_test.reshape(X_test.shape[0], 784)\n",
        "\n",
        "# Normalizing\n",
        "X = X/255.0\n",
        "X_test = X_test/255.0\n",
        "\n",
        "# Split the data X into a training set and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "#Layer dimensions\n",
        "la =[784,256,128,10]\n",
        "\n",
        "L=len(la)-1\n",
        "\n",
        "#weights and bias empty initializing\n",
        "np.random.seed(42)\n",
        "params={}"
      ],
      "metadata": {
        "id": "ydEbntHG1h5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_initialize(weight_para):\n",
        "    if weight_para == 'Xavier':\n",
        "        for i in range(0,len(la)-1):\n",
        "            params[\"W\"+str(i)]=np.random.randn(la[i+1],la[i])*np.sqrt(2/la[i+1])\n",
        "            params[\"b\"+str(i)] = np.zeros((la[i+1], 1))\n",
        "    if weight_para == 'random':\n",
        "        for i in range(0,len(la)-1):\n",
        "            params[\"W\"+str(i)] = np.random.randn(la[i+1], la[i]) * 0.01\n",
        "            params[\"b\"+str(i)] = np.zeros((la[i+1], 1))"
      ],
      "metadata": {
        "id": "WDodQndr2D4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Activation function     \n",
        "def sigmoid(x):\n",
        "     return 1. /(1. + np.exp(-x))\n",
        "\n",
        "def activation_function(x,activation):\n",
        "    if activation=='sigmoid':\n",
        "        return (1.0 / (1.0 + np.exp(-x)))\n",
        "    elif activation=='sigmoid_derivative':\n",
        "        return (activation_function(x,'sigmoid') * (1 - activation_function(x,'sigmoid')))\n",
        "    elif activation=='relu':\n",
        "        return x * (x > 0)\n",
        "    elif activation=='relu_derivative':\n",
        "        x[x <= 0.0] = 0.0\n",
        "        x[x > 0.0] = 1.0\n",
        "        return x\n",
        "    elif activation=='tanh':\n",
        "        return np.tanh(x)\n",
        "    elif activation=='tanh_derivative':\n",
        "        return 1 - (activation_function(x,'tanh') ** 2)\n",
        "    else:\n",
        "        raise Exception(\"Invalid activation function\",activation)\n",
        "def softmax(x):\n",
        "    soft = np.zeros(x.shape)\n",
        "    for i in range(0, x.shape[1]):\n",
        "        numr = np.exp(x[:, i])\n",
        "        soft[:, i] = numr/np.sum(numr)\n",
        "    return soft\n"
      ],
      "metadata": {
        "id": "rmUh2zQw2WV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feedforward(X, params,activation_para):\n",
        "\n",
        "    L = len(params)//2  \n",
        "    A = [None]*L # activations\n",
        "    H = [None]*L # pre-activations\n",
        "     \n",
        "    k=0\n",
        "    A[0]=np.dot(params[\"W\"+str(k)],X)+params[\"b\"+str(k)]\n",
        "    H[0]=activation_function(A[k],activation_para)\n",
        "   \n",
        "    for k in range(1,L-1):\n",
        "            A[k]=np.dot(params[\"W\"+str(k)],H[k-1])+params[\"b\"+str(k)]\n",
        "            H[k]=activation_function(A[k],activation_para)\n",
        "   \n",
        "    k=L-1\n",
        "    A[k]=np.dot(params[\"W\"+str(k)],H[k-1])+params[\"b\"+str(k)]\n",
        "    H[k]=softmax(A[k])\n",
        "   \n",
        "    y=H[-1]\n",
        "    return A,H,y\n",
        "\n"
      ],
      "metadata": {
        "id": "6HqvllD72iO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def back_propagation(X_train,y_train,params,A ,H,activation_para,loss):\n",
        "    #Initialzing gradients\n",
        "    L = len(params)//2\n",
        "    #A = [None]*L # activations\n",
        "    #H = [None]*L\n",
        "    g_weights = [0]*L\n",
        "    g_biases  = [0]*L\n",
        "    g_a       = [0]*(L+1)\n",
        "    g_h       = [0]*(L+1)\n",
        "    n_samples = X_train.shape[0]  # Change depending on the dimensions of data\n",
        "   \n",
        "    # print(str(activation_para)+'_derivative')\n",
        "\n",
        "    for k in range(L-1,-1,-1): #2,1,0\n",
        "       \n",
        "        if k == L-1:\n",
        "            if loss == 'cross_entropy':\n",
        "                g_a[k] = H[k]  - y_train  # keep or remove T depending on the dimensions of data\n",
        "            elif loss == 'square_loss':\n",
        "                g_a[k] = (H[k] - y_train) * H[k] * (1 - H[k])\n",
        "           \n",
        "       \n",
        "        else:\n",
        "            g_h[k] = (1/n_samples)*np.dot(params[\"W\"+str(k+1)].T,g_a[k+1])\n",
        "            if activation_para == 'sigmoid':\n",
        "                g_a[k] = (1/n_samples)*np.multiply(g_h[k],activation_function(A[k], str(activation_para)+'_derivative'))\n",
        "            elif activation_para == 'tanh':\n",
        "                g_a[k] = (1/n_samples)*np.multiply(g_h[k],activation_function(A[k], str(activation_para)+'_derivative'))\n",
        "            elif  activation_para == 'relu':\n",
        "                g_a[k] = (1/n_samples)*np.multiply(g_h[k],activation_function(A[k], str(activation_para)+'_derivative'))\n",
        "\n",
        "\n",
        "        if k == 0:\n",
        "            g_weights[k] = (1/n_samples)*np.dot(g_a[k],X_train.T)\n",
        "        else:\n",
        "            g_weights[k] = (1/n_samples)*np.dot(g_a[k],H[k-1].T)\n",
        "\n",
        "        g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "    return g_weights,g_biases"
      ],
      "metadata": {
        "id": "bhShj3k52vvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_accuracy(x,params,y,t,loss,activation_para, n_class=10,n_examples = 54000):\n",
        "    _,_,yhate=feedforward(x, params,activation_para)\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "    # ls=[]\n",
        "    # accuracy=[]\n",
        "    if loss == 'cross_entropy':\n",
        "        one_hot = np.zeros((n_class, x.shape[1]))\n",
        "        one_hot[y, np.array(list(range(x.shape[1])))] = 1\n",
        "        ls= -np.sum(one_hot *np.log(yhate))/x.shape[1]\n",
        "                  \n",
        "    if loss == 'squared_loss':\n",
        "        eIndicator = np.zeros((n_class, n_examples))\n",
        "        eIndicator[y, np.arange(n_examples)] = 1\n",
        "        ls =np.sum((yhate - eIndicator)**2) / n_examples\n",
        "    \n",
        "    yh=np.argmax(yhate,axis=0)\n",
        "    accuracy = accuracy_score(y, yh)*100 \n",
        "     \n",
        "    # for p in range(len(ls)):\n",
        "    print(\"loss of \" +str(t) +\" = \" + str(ls) )\n",
        "    print(\"Accuracy of \" +str(t) +\" = \" + str(accuracy) )\n",
        "    return accuracy,ls"
      ],
      "metadata": {
        "id": "TC402vuG3OMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vanillagd(x_train,y_train,x_val,y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para):\n",
        "    t=0\n",
        "    A=[]\n",
        "    H=[]\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "                    X_mini=x_train[(i*batch):((i+1)*batch)]\n",
        "                    y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "                    batch1 = X_mini.shape[0]\n",
        "       \n",
        "                    y_mini_one_hot = np.zeros((10, batch1))\n",
        "                    y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "                    A,H,y_hat= feedforward(X_mini.T,params,activation_para)\n",
        "                    g_weights,g_biases = back_propagation(X_mini.T,y_mini_one_hot,params,A ,H,activation_para,loss)\n",
        "                    for j in range(0,L):\n",
        "                      g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                      params['W'+str(j)]-=eta*g_weights[j]\n",
        "                      params['b'+str(j)]-=eta*g_biases[j]\n",
        "                   \n",
        "        t+=1\n",
        "   \n",
        "        train_acc,train_loss = loss_accuracy(x_train.T, params, y_train, t, loss,activation_para)\n",
        "       \n",
        "        val_acc,val_loss = loss_accuracy(x_val.T, params, y_val, t, loss,activation_para)  \n",
        "        return train_acc,train_loss,val_acc,val_loss\n",
        "       "
      ],
      "metadata": {
        "id": "Ug828GJw3Z7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MOMENTUM_GD optimizer \n",
        "        \n",
        "def momentumGD(x_train,y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay):\n",
        "    preW = {}\n",
        "    preb = {}\n",
        "    gamma = 0.9\n",
        "   \n",
        "    for k in range(0,L):\n",
        "        preW['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        preb['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "    t=0\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "            X_mini=x_train[(i*batch):((i+1)*batch)]\n",
        "            y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params)\n",
        "            g_weights,g_biases = back_propagation(X_mini.T,y_mini_one_hot,params,A ,H)\n",
        "            for j in range(0,L):\n",
        "                g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                w=(gamma*preW['W'+str(j)])+eta*g_weights[j]\n",
        "                b=(gamma*preb['b'+str(j)])+eta*g_biases[j]\n",
        "                params['W'+str(j)]-=w\n",
        "                params['b'+str(j)]-=b\n",
        "                preW['W'+str(j)]=w\n",
        "                preb['b'+str(j)]=b\n",
        "        t+=1\n",
        "        \n",
        "        #Loss & accuracy\n",
        "        print('completed feed and back per epoch')\n",
        "        loss_accuracy(x_train.T,params,y_train,t,loss)\n"
      ],
      "metadata": {
        "id": "CTdAMmye3fzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NAG optimizer  \n",
        "       \n",
        "def nag(x_train,y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay):\n",
        "    t=0\n",
        "    # batch=600\n",
        "    gamma = 0.9\n",
        "    preW={}\n",
        "    preb={}\n",
        "    for k in range(0,L):\n",
        "        preW['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        preb['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "        # print(preW['W'+str(k)].shape)\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "            X_mini=x_train[(i*batch):((i+1)*batch)]\n",
        "            y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params)\n",
        "            g_weights,g_biases = back_propagation(X_mini.T,y_mini_one_hot,params,A ,H)\n",
        "            l_params={}\n",
        "            for j in range(0,L):\n",
        "                l_params['W'+str(j)]=params['W'+str(j)]-(gamma*preW['W'+str(j)])\n",
        "                l_params['b'+str(j)]=params['b'+str(j)]-(gamma*preb['b'+str(j)])\n",
        "            for j in range(0,L):\n",
        "                g_weights,g_biases = back_propagation(X_mini.T,y_mini_one_hot,l_params,A ,H)\n",
        "                g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                w=(gamma*preW['W'+str(j)])+eta*g_weights[j]\n",
        "                b=(gamma*preb['b'+str(j)])+eta*g_biases[j]\n",
        "                params['W'+str(j)]-=w\n",
        "                params['b'+str(j)]-=b\n",
        "                preW['W'+str(j)]=w\n",
        "                preb['b'+str(j)]=b\n",
        "        t+=1\n",
        "       \n",
        "        #Loss & accuracy            \n",
        "        print('completed feed and back per epoch')\n",
        "        loss_accuracy(x_train.T,params,y_train,t,loss)\n",
        "       "
      ],
      "metadata": {
        "id": "Z6JO96vW3mMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RMSprop optimizer\n",
        "        \n",
        "def rms(x_train,y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay):\n",
        "    t=0\n",
        "    gamma = 0.9\n",
        "    eps = 1e-8\n",
        "    beta = 0.9\n",
        "    preW={}\n",
        "    preb={}\n",
        "    \n",
        "    for k in range(0,L):\n",
        "        preW['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        preb['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "            X_mini=X_train[(i*batch):((i+1)*batch)]\n",
        "            y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params)\n",
        "            g_weights,g_biases=back_propagation(X_mini.T,y_mini_one_hot,params,A ,H)  \n",
        "            for j in range(0,L):\n",
        "                g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                w=(beta*preW['W'+str(j)])+(1.0-beta)*(g_weights[j])**2\n",
        "                b=(beta*preb['b'+str(j)])+(1.0-beta)*(g_biases[j])**2\n",
        "                params['W'+str(j)]-=((eta/(np.sqrt(w+eps)))*g_weights[j])\n",
        "                params['b'+str(j)]-=((eta/(np.sqrt(b+eps)))*g_biases[j])\n",
        "                preW['W'+str(j)]=w\n",
        "                preb['b'+str(j)]=b\n",
        "        t+=1\n",
        "        \n",
        "        #Loss & accuracy\n",
        "        print('completed feed and back per epoch')\n",
        "        loss_accuracy(x_train.T,params,y_train,t,loss)\n"
      ],
      "metadata": {
        "id": "LseaKDP_3t2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STOCHASTIC optimizer\n",
        "       \n",
        "def sgd(x_train,y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay):\n",
        "    t=0\n",
        "    while t<epochs:\n",
        "        indexes=np.random.permutation(len(x_train))\n",
        "        X_ran=x_train[indexes]\n",
        "        Y_ran=y_train[indexes]\n",
        "        for x,y in zip(X_ran,Y_ran):\n",
        "            X_mini=np.array([x])\n",
        "            y_mini=np.array([y])\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params)\n",
        "            g_weights,g_biases=back_propagation(X_mini.T,y_mini_one_hot,params,A ,H)\n",
        "            for j in range(0,L):\n",
        "                g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                params['W'+str(j)]-=eta*g_weights[j]\n",
        "                params['b'+str(j)]-=eta*g_biases[j]\n",
        "        t+=1\n",
        "       \n",
        "        #Loss & accuracy\n",
        "        print('completed feed and back per epoch')\n",
        "        loss_accuracy(x_train.T,params,y_train,t,loss)\n",
        "     "
      ],
      "metadata": {
        "id": "JFp8tPoM32BJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}