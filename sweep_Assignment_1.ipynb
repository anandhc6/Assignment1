{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandhc6/Assignment1/blob/main/sweep_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_iRYe-nB_As",
        "outputId": "74638d39-1bcd-42ca-f5ee-af0853a1d6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.10)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.6)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "CknAiLRpXVFM",
        "outputId": "bd8f2c59-3b18-4f7b-932b-0717ba32ac04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep/runs/54zm7e3u\" target=\"_blank\">class-samples-1</a></strong> to <a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep/sweeps/1ek1vkmz\" target=\"_blank\">https://wandb.ai/anandh/CS6910_Assignment1_Sweep/sweeps/1ek1vkmz</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f4b63878410>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep/runs/54zm7e3u?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "#!wandb login --relogin\n",
        "wandb.init(project='CS6910_Assignment1_Sweep',entity=\"anandh\", name = 'class-samples-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ydEbntHG1h5d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from keras.datasets import fashion_mnist\n",
        "#import the required libraries\n",
        "from matplotlib import pyplot\n",
        "import math as math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "(X, y), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Reshaping the data \n",
        "X = X.reshape(X.shape[0], 784)\n",
        "X_test = X_test.reshape(X_test.shape[0], 784)\n",
        "\n",
        "# Normalizing\n",
        "X = X/255.0\n",
        "X_test = X_test/255.0\n",
        "\n",
        "# Split the data X into a training set and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "#Layer dimensions\n",
        "#la =[784,128,128,10]\n",
        "#la=[784, 256, 256, 256, 10]\n",
        "#la =[]\n",
        "#L= len(la)-1  \n",
        "#L=[]\n",
        "\n",
        "#weights and bias empty initializing\n",
        "np.random.seed(42)\n",
        "params={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "WDodQndr2D4e"
      },
      "outputs": [],
      "source": [
        "def weight_initialize(weight_para,la):\n",
        "    global params\n",
        "    params ={}\n",
        "    if weight_para == 'Xavier':\n",
        "        for i in range(0,len(la)-1):\n",
        "            params[\"W\"+str(i)]=np.random.randn(la[i+1],la[i])*np.sqrt(2/la[i+1])\n",
        "            params[\"b\"+str(i)] = np.zeros((la[i+1], 1))\n",
        "    if weight_para == 'random':\n",
        "        for i in range(0,len(la)-1):\n",
        "            params[\"W\"+str(i)] = np.random.randn(la[i+1], la[i]) * 0.01\n",
        "            params[\"b\"+str(i)] = np.zeros((la[i+1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "rmUh2zQw2WV3"
      },
      "outputs": [],
      "source": [
        "#Activation function     \n",
        "def sigmoid(x):\n",
        "     return 1. /(1. + np.exp(-x))\n",
        "\n",
        "def activation_function(x,activation):\n",
        "    if activation=='sigmoid':\n",
        "        return (1.0 / (1.0 + np.exp(-x)))\n",
        "    elif activation=='sigmoid_derivative':\n",
        "        return (activation_function(x,'sigmoid') * (1 - activation_function(x,'sigmoid')))\n",
        "    elif activation=='relu':\n",
        "        return x * (x > 0)\n",
        "    elif activation=='relu_derivative':\n",
        "        x[x <= 0.0] = 0.0\n",
        "        x[x > 0.0] = 1.0\n",
        "        return x\n",
        "    elif activation=='tanh':\n",
        "        return np.tanh(x)\n",
        "    elif activation=='tanh_derivative':\n",
        "        return 1 - (activation_function(x,'tanh') ** 2)\n",
        "    else:\n",
        "        raise Exception(\"Invalid activation function\",activation)\n",
        "def softmax(x):\n",
        "    soft = np.zeros(x.shape)\n",
        "    for i in range(0, x.shape[1]):\n",
        "        numr = np.exp(x[:, i])\n",
        "        soft[:, i] = numr/np.sum(numr)\n",
        "    return soft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "6HqvllD72iO5"
      },
      "outputs": [],
      "source": [
        "def feedforward(X, params,activation_para):\n",
        "\n",
        "    L = len(params)//2  \n",
        "    A = [None]*L # activations\n",
        "    H = [None]*L # pre-activations\n",
        "     \n",
        "    k=0\n",
        "    A[0]=np.dot(params[\"W\"+str(k)],X)+params[\"b\"+str(k)]\n",
        "    H[0]=activation_function(A[k],activation_para)\n",
        "   \n",
        "    for k in range(1,L-1):\n",
        "            A[k]=np.dot(params[\"W\"+str(k)],H[k-1])+params[\"b\"+str(k)]\n",
        "            H[k]=activation_function(A[k],activation_para)\n",
        "   \n",
        "    k=L-1\n",
        "    A[k]=np.dot(params[\"W\"+str(k)],H[k-1])+params[\"b\"+str(k)]\n",
        "    H[k]=softmax(A[k])\n",
        "   \n",
        "    y=H[-1]\n",
        "    return A,H,y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "bhShj3k52vvX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def back_propagation(X_train,y_train,params,A ,H,activation_para,loss):\n",
        "    #Initialzing gradients\n",
        "    L = len(params)//2\n",
        "    #A = [None]*L # activations\n",
        "    #H = [None]*L\n",
        "    g_weights = [0]*L\n",
        "    g_biases  = [0]*L\n",
        "    g_a       = [0]*(L+1)\n",
        "    g_h       = [0]*(L+1)\n",
        "    n_samples = X_train.shape[0]  # Change depending on the dimensions of data\n",
        "   \n",
        "    # print(str(activation_para)+'_derivative')\n",
        "\n",
        "    for k in range(L-1,-1,-1): #2,1,0\n",
        "       \n",
        "        if k == L-1:\n",
        "            if loss == 'cross_entropy':\n",
        "                g_a[k] = H[k]  - y_train  # keep or remove T depending on the dimensions of data\n",
        "            elif loss == 'square_loss':\n",
        "                g_a[k] = (H[k] - y_train) * H[k] * (1 - H[k])\n",
        "           \n",
        "       \n",
        "        else:\n",
        "            g_h[k] = (1/n_samples)*np.dot(params[\"W\"+str(k+1)].T,g_a[k+1])\n",
        "            if activation_para == 'sigmoid':\n",
        "                g_a[k] = (1/n_samples)*np.multiply(g_h[k],activation_function(A[k], str(activation_para)+'_derivative'))\n",
        "            elif activation_para == 'tanh':\n",
        "                g_a[k] = (1/n_samples)*np.multiply(g_h[k],activation_function(A[k], str(activation_para)+'_derivative'))\n",
        "            elif  activation_para == 'relu':\n",
        "                g_a[k] = (1/n_samples)*np.multiply(g_h[k],activation_function(A[k], str(activation_para)+'_derivative'))\n",
        "\n",
        "\n",
        "        if k == 0:\n",
        "            g_weights[k] = (1/n_samples)*np.dot(g_a[k],X_train.T)\n",
        "        else:\n",
        "            g_weights[k] = (1/n_samples)*np.dot(g_a[k],H[k-1].T)\n",
        "\n",
        "        g_biases[k]  = (1/n_samples)*np.sum(g_a[k], axis=1, keepdims = True)\n",
        "    return g_weights,g_biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "gYk3ztdRZ6S4"
      },
      "outputs": [],
      "source": [
        "def accuracy(x, y,params,activation_para):\n",
        "    _,_,yhate=feedforward(x, params,activation_para)\n",
        "    yh=np.argmax(yhate,axis=0)\n",
        "    accuracy = accuracy_score(y, yh)*100\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "TC402vuG3OMF"
      },
      "outputs": [],
      "source": [
        "def loss_accuracy(x,params,y,t,loss,activation_para, n_class=10):\n",
        "    _,_,yhate=feedforward(x, params,activation_para)\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "    # ls=[]\n",
        "    # accuracy=[]\n",
        "    if loss == 'cross_entropy':\n",
        "        one_hot = np.zeros((n_class, x.shape[1]))\n",
        "        one_hot[y, np.array(list(range(x.shape[1])))] = 1\n",
        "        ls= -np.sum(one_hot *np.log(yhate))/x.shape[1]\n",
        "                  \n",
        "    if loss == 'squared_loss':\n",
        "        eIndicator = np.zeros((n_class, x.shape[1]))\n",
        "        eIndicator[y, np.arange(x.shape[1])] = 1\n",
        "        ls =np.sum((yhate - eIndicator)**2) / x.shape[1]\n",
        "    \n",
        "    yh=np.argmax(yhate,axis=0)\n",
        "    accuracy = accuracy_score(y, yh)*100 \n",
        "     \n",
        "    # for p in range(len(ls)):\n",
        "    #print(\"Accuracy of  = \" + str(accuracy) +\"loss of \" +str(t) +\" = \" + str(ls) )\n",
        "    #print(\"Accuracy of \" +str(t) +\" = \" + str(accuracy) )\n",
        "    return accuracy,ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Ug828GJw3Z7U"
      },
      "outputs": [],
      "source": [
        "def vanillagd(x_train,y_train,x_val,y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L):\n",
        "    t=0\n",
        "    A=[]\n",
        "    H=[]\n",
        "    train_acc=[]\n",
        "    train_loss=[]\n",
        "    val_acc=[]\n",
        "    val_loss=[]\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "                    X_mini=x_train[(i*batch):((i+1)*batch)]\n",
        "                    y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "                    batch1 = X_mini.shape[0]\n",
        "       \n",
        "                    y_mini_one_hot = np.zeros((10, batch1))\n",
        "                    y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "                    A,H,y_hat= feedforward(X_mini.T,params,activation_para)\n",
        "                    g_weights,g_biases = back_propagation(X_mini.T,y_mini_one_hot,params,A ,H,activation_para,loss)\n",
        "                    for j in range(0,L):\n",
        "                      g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                      params['W'+str(j)]-=eta*g_weights[j]\n",
        "                      params['b'+str(j)]-=eta*g_biases[j]\n",
        "                   \n",
        "        t+=1\n",
        "        train_a,train_l = loss_accuracy(x_train.T, params, y_train, t, loss,activation_para)\n",
        "        train_acc.append(train_a)\n",
        "        train_loss.append(train_l)\n",
        "        val_a,val_l = loss_accuracy(x_val.T, params, y_val, t, loss,activation_para)  \n",
        "        val_acc.append(val_a)\n",
        "        val_loss.append(val_l)\n",
        "     \n",
        "    return train_acc,train_loss,val_acc,val_loss\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "CTdAMmye3fzt"
      },
      "outputs": [],
      "source": [
        "# MOMENTUM_GD optimizer \n",
        "        \n",
        "def momentumGD(x_train,y_train,x_val,y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L):\n",
        "    preW = {}\n",
        "    preb = {}\n",
        "    gamma = 0.9\n",
        "    train_acc=[]\n",
        "    train_loss=[]\n",
        "    val_acc=[]\n",
        "    val_loss=[]\n",
        "    for k in range(0,L):\n",
        "        preW['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        preb['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "    t=0\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "            X_mini=x_train[(i*batch):((i+1)*batch)]\n",
        "            y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params,activation_para)\n",
        "            g_weights,g_biases = back_propagation(X_mini.T,y_mini_one_hot,params,A ,H,activation_para,loss)\n",
        "            for j in range(0,L):\n",
        "                #g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                w=(gamma*preW['W'+str(j)])+eta*g_weights[j]\n",
        "                b=(gamma*preb['b'+str(j)])+eta*g_biases[j]\n",
        "                params['W'+str(j)]-=w-np.multiply(eta*weight_decay,params['W'+str(j)])\n",
        "                params['b'+str(j)]-=b-np.multiply(eta*weight_decay,params['b'+str(j)])\n",
        "                preW['W'+str(j)]=w\n",
        "                preb['b'+str(j)]=b\n",
        "        t+=1\n",
        "        \n",
        "        #Loss & accuracy\n",
        "        train_a,train_l = loss_accuracy(x_train.T, params, y_train, t, loss,activation_para)\n",
        "        train_acc.append(train_a)\n",
        "        train_loss.append(train_l)\n",
        "        val_a,val_l = loss_accuracy(x_val.T, params, y_val, t, loss,activation_para)  \n",
        "        val_acc.append(val_a)\n",
        "        val_loss.append(val_l)\n",
        "    return train_acc,train_loss,val_acc,val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "Z6JO96vW3mMl"
      },
      "outputs": [],
      "source": [
        "#NAG optimizer  \n",
        "       \n",
        "def nag(x_train,y_train,x_val,y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L):\n",
        "    t=0\n",
        "    # batch=600\n",
        "    gamma = 0.9\n",
        "    preW={}\n",
        "    preb={}\n",
        "    train_acc=[]\n",
        "    train_loss=[]\n",
        "    val_acc=[]\n",
        "    val_loss=[]\n",
        "    for k in range(0,L):\n",
        "        preW['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        preb['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "        # print(preW['W'+str(k)].shape)\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "            X_mini=x_train[(i*batch):((i+1)*batch)]\n",
        "            y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params,activation_para)\n",
        "            g_weights,g_biases = back_propagation(X_mini.T,y_mini_one_hot,params,A ,H,activation_para,loss)\n",
        "            l_params={}\n",
        "            for j in range(0,L):\n",
        "                l_params['W'+str(j)]=params['W'+str(j)]-(gamma*preW['W'+str(j)])\n",
        "                l_params['b'+str(j)]=params['b'+str(j)]-(gamma*preb['b'+str(j)])\n",
        "            for j in range(0,L):\n",
        "                g_weights,g_biases = back_propagation(X_mini.T,y_mini_one_hot,l_params,A ,H,activation_para,loss)\n",
        "                #g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                w=(gamma*preW['W'+str(j)])+eta*g_weights[j]\n",
        "                b=(gamma*preb['b'+str(j)])+eta*g_biases[j]\n",
        "                params['W'+str(j)]-=w-np.multiply(eta*weight_decay,params['W'+str(j)])\n",
        "                params['b'+str(j)]-=b-np.multiply(eta*weight_decay,params['b'+str(j)])\n",
        "                preW['W'+str(j)]=w\n",
        "                preb['b'+str(j)]=b\n",
        "        t+=1\n",
        "       \n",
        "        #Loss & accuracy            \n",
        "        train_a,train_l = loss_accuracy(x_train.T, params, y_train, t, loss,activation_para)\n",
        "        train_acc.append(train_a)\n",
        "        train_loss.append(train_l)\n",
        "        val_a,val_l = loss_accuracy(x_val.T, params, y_val, t, loss,activation_para)  \n",
        "        val_acc.append(val_a)\n",
        "        val_loss.append(val_l)\n",
        "     \n",
        "    return train_acc,train_loss,val_acc,val_loss\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "LseaKDP_3t2F"
      },
      "outputs": [],
      "source": [
        "#RMSprop optimizer\n",
        "        \n",
        "def rms(x_train,y_train,x_val,y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L):\n",
        "    t=0\n",
        "    gamma = 0.9\n",
        "    eps = 1e-8\n",
        "    beta = 0.9\n",
        "    preW={}\n",
        "    preb={}\n",
        "    train_acc=[]\n",
        "    train_loss=[]\n",
        "    val_acc=[]\n",
        "    val_loss=[]\n",
        "    for k in range(0,L):\n",
        "        preW['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        preb['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "            X_mini=X_train[(i*batch):((i+1)*batch)]\n",
        "            y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params,activation_para)\n",
        "            g_weights,g_biases=back_propagation(X_mini.T,y_mini_one_hot,params,A ,H,activation_para,loss)  \n",
        "            for j in range(0,L):\n",
        "                #g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                w=(beta*preW['W'+str(j)])+(1.0-beta)*(g_weights[j])**2\n",
        "                b=(beta*preb['b'+str(j)])+(1.0-beta)*(g_biases[j])**2\n",
        "                params['W'+str(j)]-=((eta/(np.sqrt(w+eps)))*g_weights[j])-np.multiply(eta*weight_decay,params['W'+str(j)])\n",
        "                params['b'+str(j)]-=((eta/(np.sqrt(b+eps)))*g_biases[j])-np.multiply(eta*weight_decay,params['b'+str(j)])\n",
        "                preW['W'+str(j)]=w\n",
        "                preb['b'+str(j)]=b\n",
        "        t+=1\n",
        "        \n",
        "        #Loss & accuracy\n",
        "        train_a,train_l = loss_accuracy(x_train.T, params, y_train, t, loss,activation_para)\n",
        "        train_acc.append(train_a)\n",
        "        train_loss.append(train_l)\n",
        "        val_a,val_l = loss_accuracy(x_val.T, params, y_val, t, loss,activation_para)  \n",
        "        val_acc.append(val_a)\n",
        "        val_loss.append(val_l)\n",
        "    \n",
        "    return train_acc,train_loss,val_acc,val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "JFp8tPoM32BJ"
      },
      "outputs": [],
      "source": [
        "#STOCHASTIC optimizer\n",
        "       \n",
        "def sgd(x_train,y_train,x_val,y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L):\n",
        "    t=0\n",
        "    train_acc=[]\n",
        "    train_loss=[]\n",
        "    val_acc=[]\n",
        "    val_loss=[]\n",
        "    while t<epochs:\n",
        "        indexes=np.random.permutation(len(x_train))\n",
        "        X_ran=x_train[indexes]\n",
        "        Y_ran=y_train[indexes]\n",
        "        for x,y in zip(X_ran,Y_ran):\n",
        "            X_mini=np.array([x])\n",
        "            y_mini=np.array([y])\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params,activation_para)\n",
        "            g_weights,g_biases=back_propagation(X_mini.T,y_mini_one_hot,params,A ,H,activation_para,loss)\n",
        "            for j in range(0,L):\n",
        "                #g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                params['W'+str(j)]-=eta*g_weights[j]#-np.multiply(eta*weight_decay,params['W'+str(j)])\n",
        "                params['b'+str(j)]-=eta*g_biases[j]#-np.multiply(eta*weight_decay,params['b'+str(j)])\n",
        "        t+=1\n",
        "       \n",
        "        #Loss & accuracy\n",
        "        train_a,train_l = loss_accuracy(x_train.T, params, y_train, t, loss,activation_para)\n",
        "        train_acc.append(train_a)\n",
        "        train_loss.append(train_l)\n",
        "        val_a,val_l = loss_accuracy(x_val.T, params, y_val, t, loss,activation_para)  \n",
        "        val_acc.append(val_a)\n",
        "        val_loss.append(val_l) \n",
        "    return train_acc,train_loss,val_acc,val_loss\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "s-XjjRyV38h1"
      },
      "outputs": [],
      "source": [
        "#ADAM optimizer\n",
        "        \n",
        "def adam(x_train,y_train,x_val,y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L):\n",
        "    eps = 1e-8\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    t=0\n",
        "    preW={}\n",
        "    preb={}\n",
        "    mt={}\n",
        "    vt={}\n",
        "    it=0\n",
        "    train_acc=[]\n",
        "    train_loss=[]\n",
        "    val_acc=[]\n",
        "    val_loss=[]\n",
        "    for k in range(0,L):\n",
        "        preW['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        preb['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "    for k in range(0,L):\n",
        "        mt['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        vt['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "            X_mini=x_train[(i*batch):((i+1)*batch)]\n",
        "            y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params,activation_para)\n",
        "            g_weights,g_biases=back_propagation(X_mini.T,y_mini_one_hot,params,A ,H,activation_para,loss)\n",
        "            it+=1\n",
        "            for j in range(0,L):\n",
        "                #g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                mt['W'+str(j)]=beta1*mt['W'+str(j)]+(1-beta1)*g_weights[j]\n",
        "                vt['b'+str(j)]=beta1*vt['b'+str(j)]+(1-beta1)*g_biases[j]\n",
        "                preW['W'+str(j)]=beta2*preW['W'+str(j)]+(1-beta2)*(g_weights[j]**2)\n",
        "                preb['b'+str(j)]=beta2*preb['b'+str(j)]+(1-beta2)*(g_biases[j]**2)\n",
        "                mhatw=mt['W'+str(j)]/(1-(beta1)**it)\n",
        "                mhatb=vt['b'+str(j)]/(1-(beta1)**it)\n",
        "                vhatw=preW['W'+str(j)]/(1-(beta2)**it)\n",
        "                vhatb=preb['b'+str(j)]/(1-(beta2)**it)\n",
        "                params['W'+str(j)]-=((eta/(np.sqrt(vhatw+eps)))*mhatw)-np.multiply(eta*weight_decay,params['W'+str(j)])\n",
        "                params['b'+str(j)]-=((eta/(np.sqrt(vhatb+eps)))*mhatb)-np.multiply(eta*weight_decay,params['b'+str(j)])\n",
        "        t+=1\n",
        "        \n",
        "        #Loss & accuracy\n",
        "        train_a,train_l = loss_accuracy(x_train.T, params, y_train, t, loss,activation_para)\n",
        "        train_acc.append(train_a)\n",
        "        train_loss.append(train_l)\n",
        "        val_a,val_l = loss_accuracy(x_val.T, params, y_val, t, loss,activation_para)  \n",
        "        val_acc.append(val_a)\n",
        "        val_loss.append(val_l)\n",
        "     \n",
        "    return train_acc,train_loss,val_acc,val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "3_PfaI654EIg"
      },
      "outputs": [],
      "source": [
        "#NADAM optimizer\n",
        "       \n",
        "def nadam(x_train,y_train,x_val,y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L):\n",
        "\n",
        "    print('inside optimizer nadam')\n",
        "    print(la)\n",
        "    eps = 1e-8\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    t=0\n",
        "    minibatch=math.ceil(x_train.shape[0]/batch)\n",
        "    preW={}\n",
        "    preb={}\n",
        "    mt={}\n",
        "    vt={}\n",
        "    it=0\n",
        "    train_acc=[]\n",
        "    train_loss=[]\n",
        "    val_acc=[]\n",
        "    val_loss=[]\n",
        "    for k in range(0,L):\n",
        "        preW['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        preb['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "    for k in range(0,L):\n",
        "        mt['W'+str(k)]=np.zeros((la[k+1],la[k]))\n",
        "        vt['b'+str(k)]=np.zeros((la[k+1],1))\n",
        "    while t<epochs:\n",
        "        for i in range(0,minibatch,1):\n",
        "            X_mini=x_train[(i*batch):((i+1)*batch)]\n",
        "            y_mini=y_train[(i*batch):((i+1)*batch)]\n",
        "            batch1 = X_mini.shape[0]\n",
        "            y_mini_one_hot = np.zeros((10, batch1))\n",
        "            y_mini_one_hot[y_mini, np.array(list(range(batch1)))] = 1\n",
        "            A,H,y_hat=feedforward(X_mini.T,params,activation_para)\n",
        "            g_weights,g_biases=back_propagation(X_mini.T,y_mini_one_hot,params,A ,H,activation_para,loss)\n",
        "            it+=1\n",
        "            for j in range(0,L):\n",
        "                #g_weights[j]=g_weights[j]+weight_decay*params['W'+str(j)]\n",
        "                mt['W'+str(j)]=beta1*mt['W'+str(j)]+(1-beta1)*g_weights[j]\n",
        "                vt['b'+str(j)]=beta1*vt['b'+str(j)]+(1-beta1)*g_biases[j]\n",
        "                preW['W'+str(j)]=beta2*preW['W'+str(j)]+(1-beta2)*(g_weights[j]**2)\n",
        "                preb['b'+str(j)]=beta2*preb['b'+str(j)]+(1-beta2)*(g_biases[j]**2)\n",
        "                mhatw=mt['W'+str(j)]/(1-(beta1)**it)\n",
        "                mhatb=vt['b'+str(j)]/(1-(beta1)**it)\n",
        "                vhatw=preW['W'+str(j)]/(1-(beta2)**it)\n",
        "                vhatb=preb['b'+str(j)]/(1-(beta2)**it)\n",
        "                mbarw=beta1*mhatw+(1.0-beta1)*g_weights[j]\n",
        "                mbarb=beta1*mhatb+(1.0-beta1)*g_biases[j]\n",
        "                params['W'+str(j)]-=((eta/(np.sqrt(vhatw+eps)))*mbarw)-np.multiply(eta*weight_decay,params['W'+str(j)])\n",
        "                params['b'+str(j)]-=((eta/(np.sqrt(vhatb+eps)))*mbarb)-np.multiply(eta*weight_decay,params['b'+str(j)])\n",
        "        t+=1\n",
        "       \n",
        "        #Loss & accuracy\n",
        "    \n",
        "        train_a,train_l = loss_accuracy(x_train.T, params, y_train, t, loss,activation_para)\n",
        "        train_acc.append(train_a)\n",
        "        train_loss.append(train_l)\n",
        "        val_a,val_l = loss_accuracy(x_val.T, params, y_val, t, loss,activation_para)  \n",
        "        val_acc.append(val_a)\n",
        "        val_loss.append(val_l)\n",
        "    return train_acc,train_loss,val_acc,val_loss\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "4MRkq77z4Lxl"
      },
      "outputs": [],
      "source": [
        "#FIT models        \n",
        "def fit(X,y,batch,epochs,eta,weight_para,optimiser,loss,weight_decay,activation_para,la,L):      \n",
        "    weight_para = 'Xavier'\n",
        "    weight_initialize(weight_para,la)\n",
        "    \n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "    \n",
        "    #batch=100\n",
        "    \n",
        "    minibatch=math.ceil(X_train.shape[0]/batch)\n",
        "    \n",
        "    if optimiser=='momentumGD':\n",
        "        print('calling optimizer momentum ')\n",
        "        train_acc,train_loss,val_acc,val_loss=momentumGD(X_train,y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L)\n",
        "    elif optimiser=='nadam':\n",
        "        print('calling optimizer nadam ')\n",
        "        train_acc,train_loss,val_acc,val_loss=nadam(X_train,Y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L)\n",
        "    elif optimiser=='rms':\n",
        "        print('calling optimizer rms ')\n",
        "        train_acc,train_loss,val_acc,val_loss=rms(X_train,Y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L)\n",
        "    elif optimiser=='sgd':\n",
        "        print('calling optimizer sgd ')\n",
        "        train_acc,train_loss,val_acc,val_loss=sgd(X_train,Y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L)\n",
        "    elif optimiser=='adam':\n",
        "        print('calling optimizer adam ')\n",
        "        train_acc,train_loss,val_acc,val_loss=adam(X_train,Y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L)\n",
        "    elif optimiser=='nag':\n",
        "        print('calling optimizer nag ')\n",
        "        train_acc,train_loss,val_acc,val_loss=nag(X_train,Y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L)\n",
        "    elif optimiser=='vanillagd':\n",
        "        train_acc,train_loss,val_acc,val_loss=vanillagd(X_train,Y_train,X_val,Y_val,eta,epochs,minibatch,batch,loss,weight_decay,activation_para,la,L)   \n",
        "    \n",
        "    return train_acc,train_loss,val_acc,val_loss\n",
        "\n",
        "         \n",
        "#train_acc,train_loss,val_acc,val_loss= fit(X,y,100,5,0.01,'Xavier','nadam','cross_entropy',0.0005,'tanh',la,L)\n",
        "\n",
        "#print(train_acc,train_loss,val_acc,val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cd571ad2e0764fa799034c7c5f93118b",
            "744b923ecb3246319d2a4dbc838cf5b7",
            "95b91a5185194bdba4cf15320770fc23",
            "12c5710471ac4e9a9128503f1bb55743",
            "dfb52b983bea47d88d042cfb96c25982",
            "38f685a02d5443439aac6af83e3997b3",
            "5c20f6595dc442db81872abae63c18a9",
            "eea48b9554c84e5b8e643aa54caaaecf"
          ]
        },
        "id": "rxJwQw4_4hUl",
        "outputId": "4d810986-c799-40d6-9890-ee24c6008e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:54zm7e3u) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 5775... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd571ad2e0764fa799034c7c5f93118b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "</div><div class=\"wandb-col\">\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">class-samples-1</strong>: <a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep/runs/54zm7e3u\" target=\"_blank\">https://wandb.ai/anandh/CS6910_Assignment1_Sweep/runs/54zm7e3u</a><br/>\n",
              "Find logs at: <code>./wandb/run-20220225_072919-54zm7e3u/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:54zm7e3u). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep/runs/54zm7e3u\" target=\"_blank\">class-samples-1</a></strong> to <a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep/sweeps/1ek1vkmz\" target=\"_blank\">https://wandb.ai/anandh/CS6910_Assignment1_Sweep/sweeps/1ek1vkmz</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 4kdj2a9g\n",
            "Sweep URL: https://wandb.ai/anandh/CS6910_Assignment1_Sweep/sweeps/4kdj2a9g\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qhueh6wu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_para: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teta: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_of_hidden: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_of_neurons: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_para: random\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep/runs/qhueh6wu\" target=\"_blank\">devout-sweep-1</a></strong> to <a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/anandh/CS6910_Assignment1_Sweep/sweeps/4kdj2a9g\" target=\"_blank\">https://wandb.ai/anandh/CS6910_Assignment1_Sweep/sweeps/4kdj2a9g</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[784, 256, 256, 256, 256, 256, 10]\n",
            "calling optimizer nadam \n",
            "inside optimizer nadam\n",
            "[784, 256, 256, 256, 256, 256, 10]\n",
            "(784, 54000)\n",
            "(54000,)\n",
            "(784, 6000)\n",
            "(6000,)\n",
            "(784, 54000)\n",
            "(54000,)\n",
            "(784, 6000)\n",
            "(6000,)\n",
            "(784, 54000)\n",
            "(54000,)\n",
            "(784, 6000)\n",
            "(6000,)\n",
            "(784, 54000)\n",
            "(54000,)\n",
            "(784, 6000)\n",
            "(6000,)\n",
            "(784, 54000)\n",
            "(54000,)\n",
            "(784, 6000)\n",
            "(6000,)\n",
            "(784, 54000)\n",
            "(54000,)\n",
            "(784, 6000)\n",
            "(6000,)\n",
            "(784, 54000)\n",
            "(54000,)\n",
            "(784, 6000)\n",
            "(6000,)\n",
            "(784, 54000)\n",
            "(54000,)\n",
            "(784, 6000)\n",
            "(6000,)\n"
          ]
        }
      ],
      "source": [
        "features=784\n",
        "n_class=10\n",
        "loss='cross_entropy'\n",
        "#la=[]\n",
        "#L=0\n",
        "#print(\"calling FF_fit\")\n",
        "def FF_fit():\n",
        "    \"\"\"\n",
        "    This function is used for the hyperparameter tuning using wandb sweeps\n",
        "    \"\"\"\n",
        "    # Default values for hyper-parameters\n",
        "    config_defaults = {\n",
        "        'epochs': 10,\n",
        "        'batch': 64,\n",
        "        'eta': 1e-3,\n",
        "        'activation_para': 'relu',\n",
        "        'optimiser': 'adam',\n",
        "        'weight_para': 'Xavier',\n",
        "        'weight_decay': 0,\n",
        "        'no_of_neurons': 64,\n",
        "        'no_of_hidden': 3\n",
        "    }\n",
        "    \n",
        "    la=[]\n",
        "    L=0\n",
        "    #wandb.init()\n",
        "    wandb.init(config=config_defaults)\n",
        "    config = wandb.config\n",
        "    \n",
        "    no_of_neurons = config.no_of_neurons\n",
        "    no_of_hidden = config.no_of_hidden\n",
        "    weight_para = config.weight_para\n",
        "    epochs = config.epochs\n",
        "    batch = config.batch\n",
        "    eta = config.eta\n",
        "    activation_para = config.activation_para\n",
        "    weight_decay = config.weight_decay\n",
        "    optimiser = config.optimiser\n",
        "    \n",
        "    \n",
        "    la =[features]+[no_of_neurons]*no_of_hidden+[n_class]\n",
        "    print(la)\n",
        "    L =len(la)-1\n",
        "    \n",
        "    name = \"lr_\"+str(config.eta)+\"_hl_\"+str(config.no_of_hidden)+\"_bs_\"+str(config.batch)+\"_ac_\"+str(config.activation_para)+\"_ep_\"+str(config.epochs)\n",
        "    wandb.run.name = name\n",
        "    # name = \"lr_{}_af_{}_in_{}_op_{}_b_{}_wd_{}_ep_{}_nn_{}_nh_{}\".format(eta, activation_para, weight_para, optimiser, batch, weight_decay, epochs, no_of_neurons, no_of_hidden)\n",
        "    # print(name)\n",
        "    # fit(X,y,100,5,0.01,'Xavier','nadam','cross_entropy',0.0005,'tanh')\n",
        "    train_acc,train_loss,val_acc,val_loss=fit(X,y,batch,epochs,eta,weight_para,optimiser,loss,weight_decay,activation_para,la,L)\n",
        "\n",
        "    #printing Accuracy and loss\n",
        "    for p in range(len(train_acc)):\n",
        "       print(\"Accuracy of  train= \" + str(train_acc[p]) +\"   Loss of train = \" + str(train_loss[p]) + \"   Accuracy of val= \" + str(val_acc[p]) + \"  Accuracy of val= \" + str(val_loss[p]))\n",
        "\n",
        "    test_acc=accuracy(X_test.T,y_test,params,activation_para)\n",
        "    print(\"Accuracy of  test_data= \" + str(test_acc))\n",
        "\n",
        "    for i in range(len(train_acc)):\n",
        "      wandb.log({'train_acc':train_acc[i], 'train_loss':train_loss[i], 'val_acc':val_acc[i] ,'val_loss':val_loss[i], 'epochs': i+1})\n",
        "    wandb.log({'test_acc' : test_acc})\n",
        "    wandb.run.save()\n",
        "    wandb.run.finish()\n",
        "    \n",
        "sweep_config = {\n",
        "    'method': 'bayes', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'val_loss',\n",
        "      'goal': 'minimize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [10,20,40]\n",
        "        },\n",
        "        'no_of_hidden': {\n",
        "            'values': [3,4,5]\n",
        "        },\n",
        "        'no_of_neurons': {\n",
        "            'values': [128,256]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0,0.0005]\n",
        "        },\n",
        "        'eta': {\n",
        "            'values': [ 0.01,0.1,0.3]\n",
        "        },\n",
        "        'optimiser': {\n",
        "            'values': ['rms','momentumGD','nag','sgd','adam','nadam']\n",
        "        },\n",
        "        'batch':{\n",
        "            'values': [16,32,64,100]\n",
        "        },\n",
        "        'weight_para':{\n",
        "            'values': ['Xavier','random']\n",
        "        },\n",
        "        'activation_para': {\n",
        "            'values': ['tanh','sigmoid','relu']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "    \n",
        "wandb.init(project='cs6910-assignment1', name = 'class-samples-1')\n",
        "#wandb.init(entity=\"anandh\", project = \"CS6910_Assignment1_Sweep\")\n",
        "sweep_id=wandb.sweep(sweep_config,entity=\"anandh\", project = \"CS6910_Assignment1_Sweep\")\n",
        "#sweep_id= \"w7mehhhy\"\n",
        "#%start wandb sweep\n",
        "wandb.agent(sweep_id, FF_fit, count = 1)\n",
        "#wandb.agent(sweep_id, FF_fit, entity=\"anandh\", project = \"CS6910_Assignment1_Sweep\", count = 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VJEhLatX0BKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_mnist_full, y_mnist_full), (X_test_mnist_full, y_test_mnist_full) = mnist.load_data()\n",
        "\n",
        "# Reshaping the data \n",
        "X_mnist_full = X.reshape(X.shape[0], 784)\n",
        "X_test_mnist_full = X_test.reshape(X_test.shape[0], 784)\n",
        "\n",
        "# Normalizing\n",
        "X_mnist_full = X_mnist_full/255.0\n",
        "X_test_mnist_full = X_test_mnist_full/255.0\n",
        "\n",
        "#layer_dimensions\n",
        "la_mnist_full=[784, 256, 256, 256, 10]\n",
        "L_mnist_full= len(la)-1\n",
        "\n",
        "# Split the data X into a training set and validation set\n",
        "X_train_mnist_full, X_val_mnist_full, y_train_mnist_full, y_val_mnist_full = train_test_split(X_mnist_full, y_mnist_full, test_size=0.1, random_state=42)\n",
        "\n",
        "train_acc_mnist_full,train_loss_mnist_full,val_acc_mnist_full,val_loss_mnist_full= fit(X_mnist_full,y_mnist_full,100,5,0.01,'Xavier','nadam','cross_entropy',0.0005,'tanh',la_mnist_full,L_mnist_full)\n",
        "\n",
        "#train_acc,train_loss,val_acc,val_loss=fit(X,y,batch,epochs,eta,weight_para,optimiser,loss,weight_decay,activation_para)\n",
        "\n",
        "#printing Accuracy and loss\n",
        "for p in range(train_acc_mnist_full):\n",
        "  print(\"Accuracy of  train= \" + str(train_acc_mnist_full[p]) +\"Loss of train = \" + str(train_loss_mnist_full[p]) + \"Accuracy of val=\" + str(val_acc_mnist_full[p]) + \"Accuracy of val=\" + str(val_loss_mnist_full[p]))\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "JDKQbRVPa735"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sweep Assignment 1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd571ad2e0764fa799034c7c5f93118b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_744b923ecb3246319d2a4dbc838cf5b7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_95b91a5185194bdba4cf15320770fc23",
              "IPY_MODEL_12c5710471ac4e9a9128503f1bb55743"
            ]
          }
        },
        "744b923ecb3246319d2a4dbc838cf5b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95b91a5185194bdba4cf15320770fc23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_dfb52b983bea47d88d042cfb96c25982",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38f685a02d5443439aac6af83e3997b3"
          }
        },
        "12c5710471ac4e9a9128503f1bb55743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5c20f6595dc442db81872abae63c18a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eea48b9554c84e5b8e643aa54caaaecf"
          }
        },
        "dfb52b983bea47d88d042cfb96c25982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38f685a02d5443439aac6af83e3997b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c20f6595dc442db81872abae63c18a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eea48b9554c84e5b8e643aa54caaaecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}